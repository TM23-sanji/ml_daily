{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TKEwf7I9SaDa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as k\n",
        "import sklearn as sk\n",
        "from functools import partial\n",
        "import tensorflow_datasets as tfds\n",
        "from pathlib import Path\n",
        "import os\n",
        "import tensorflow_hub as hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
        "  raw_text = f.read()\n",
        "print('Total num of characters:', len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "LgQ5lFCuUk4N",
        "outputId": "dacd15dd-2d5d-4862-c157-d63421665600"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bd7f5646-ba68-468b-960d-393243a96981\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bd7f5646-ba68-468b-960d-393243a96981\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving the-verdict.txt to the-verdict.txt\n",
            "Total num of characters: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = 'Hello, world. This, is a test.'\n",
        "result = re.split(r'(\\s)', text)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8Iek9CMYkI5",
        "outputId": "4c325b6c-5411-4d7f-ce33-8238c20ccae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.split(r'([,.]|\\s)', text)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26JsU7PKa5sF",
        "outputId": "37dc168a-37ac-4011-d346-ed21e17e314c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " ',',\n",
              " '',\n",
              " ' ',\n",
              " 'world',\n",
              " '.',\n",
              " '',\n",
              " ' ',\n",
              " 'This',\n",
              " ',',\n",
              " '',\n",
              " ' ',\n",
              " 'is',\n",
              " ' ',\n",
              " 'a',\n",
              " ' ',\n",
              " 'test',\n",
              " '.',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = [item for item in result if item.strip()]\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB28ls5GbXdM",
        "outputId": "f5db8999-2045-42f4-d872-0ed09e88ac79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Hello, world. Is this-- a test?'\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG08lt_2biK3",
        "outputId": "c2dcc134-4579-4510-eae0-036f162812dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(len(preprocessed))\n",
        "preprocessed[:30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtSwvrs8dbDw",
        "outputId": "4d46d627-2676-44f8-9ba8-e5f2d649bde3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'HAD',\n",
              " 'always',\n",
              " 'thought',\n",
              " 'Jack',\n",
              " 'Gisburn',\n",
              " 'rather',\n",
              " 'a',\n",
              " 'cheap',\n",
              " 'genius',\n",
              " '--',\n",
              " 'though',\n",
              " 'a',\n",
              " 'good',\n",
              " 'fellow',\n",
              " 'enough',\n",
              " '--',\n",
              " 'so',\n",
              " 'it',\n",
              " 'was',\n",
              " 'no',\n",
              " 'great',\n",
              " 'surprise',\n",
              " 'to',\n",
              " 'me',\n",
              " 'to',\n",
              " 'hear',\n",
              " 'that',\n",
              " ',',\n",
              " 'in']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qfuvHqLd8uv",
        "outputId": "6accd1cb-29ff-4d7a-f3d2-124d15af1a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
        "for i, item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i >=50:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVsEEnzDezEy",
        "outputId": "ae588734-2f73-475b-aeb6-c8470b58a537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [\n",
        "        item.strip() for item in preprocessed if item.strip()\n",
        "    ]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "G3PeRnk_gJop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7atY5Z7micIy",
        "outputId": "60cb6427-a22d-4b38-96ab-9c91602eee4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6UrTIgWrhcQ",
        "outputId": "63dfe007-e51a-4174-c941-0150a82bee5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
        "print(len(vocab.items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TWZjd2fsDDl",
        "outputId": "1bdce0d0-2c8c-4894-dd8f-be6290da2a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL-aFe1EzTBa",
        "outputId": "8187ca75-f2ec-4e48-e7e1-98dcdc944669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|endoftext|>', 1130)\n",
            "('<|unk|>', 1131)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [\n",
        "        item.strip() for item in preprocessed if item.strip()\n",
        "    ]\n",
        "    preprocessed = [item if item in self.str_to_int\n",
        "                    else \"<|unk|>\" for item in preprocessed]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "0q_8u-SRzaP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4YcJnla1QZy",
        "outputId": "f2a6d406-3f97-4959-d671-e86a85446566"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "print(tokenizer.encode(text))\n",
        "print(tokenizer.decode(tokenizer.encode(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV3Ledxx1T7t",
        "outputId": "be83d879-54ee-4e8a-ab25-44c8223ce409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
            "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Byte Pair Encoding**\n",
        "\n",
        "- freq of words like ('low' : 5, 'lower' : 2)\n",
        "- target vocab size : n\n",
        "- word tokenization : \"low\" → ('l', 'o', 'w', 'end_of_word')     frequency: 5\n",
        "- character level vocab : 'l': 0, 'o': 1, 'w': 2, 'e': 3, 'r': 4, 'n': 5,\n",
        "'s': 6, 'end_of_word': 10\n",
        "- most frequent pair ('e', 's')\n",
        "- create new token 'es' (token ID: 11)\n",
        "- update word representation : \"newest\" → ('n', 'e', 'w', 'es', 't', 'end_of_word')     frequency: 6\n",
        "- next Most frequent pair: ('es', 't') with 9 occurrences\n",
        "- Create new token 'est' (token ID: 12)\n",
        "- update word : \"newest\" → ('n', 'e', 'w', 'est', 'end_of_word')     frequency: 6\n",
        "- keep on going until reach the target_vocab_size"
      ],
      "metadata": {
        "id": "t7nO_frk_x9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We have a big corpus of words now we take each alphabet and neighbour and merge acc to the most frequent, we keep on doing that until vocab_size. Then assign a token ids to each merged token & individuals that are left. This wasy our tokenizer is trained. Now when a new sentence arrives, if our tokenizer has a id for 'run' but word is 'runner' than ['run', 'n', 'e', 'r'] then their ids [100, 5, 6, 9]**"
      ],
      "metadata": {
        "id": "NlSxZ0TjUF4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Core data types used throughout\n",
        "# WordFrequencies = Dict[Tuple[str, ...], int]  # ('h', 'e', 'l', 'l', 'o', '</w>') -> 5\n",
        "# PairCounts = Dict[Tuple[str, str], int]       # ('l', 'l') -> 10\n",
        "# Vocabulary = Dict[str, int]                   # 'hello' -> 42\n",
        "# MergeRules = List[Tuple[Tuple[str, str], str]] # [((('l', 'l'), 'll')), ...]\n",
        "# TokenSequence = List[str]                     # ['he', 'll', 'o</w>']\n",
        "# TokenIDs = List[int]                          # [1, 15, 23]"
      ],
      "metadata": {
        "id": "jUEeV3dY1yXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(self, end_of_word_token: str = \"</w>\", lowercase: bool = True):\n",
        "        self.end_of_word_token = end_of_word_token\n",
        "        self.lowercase = lowercase\n",
        "\n",
        "    def normalize_text(self, text: str) -> str:\n",
        "        # Clean and standardize text (lowercase, remove special chars, etc.)\n",
        "        table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "        clean_text = text.translate(table)\n",
        "        return clean_text\n",
        "\n",
        "    def tokenize_to_words(self, text: str) -> list[str]:\n",
        "        # Split text into individual words\n",
        "        norm = self.normalize_text(text)\n",
        "        if self.lowercase:\n",
        "          norm = norm.lower()\n",
        "        return norm.split()\n",
        "\n",
        "    def words_to_char_tuples(self, words: list[str]) -> list[tuple[str, ...]]:\n",
        "        # Convert words to character sequences with end-of-word markers\n",
        "        return [tuple(list(word) + [self.end_of_word_token]) for word in words]\n",
        "\n",
        "    def build_word_frequencies(self, corpus: list[str]) -> dict[tuple[str, ...], int]:\n",
        "        # Count how often each word appears in corpus\n",
        "        words = []\n",
        "        for text in corpus:\n",
        "          words.extend(self.tokenize_to_words(text))\n",
        "\n",
        "        char_tuples = self.words_to_char_tuples(words)\n",
        "        return Counter(char_tuples)\n",
        "\n",
        "    def preprocess_corpus(self, corpus: list[str]) -> dict[tuple[str, ...], int]:\n",
        "        # Main entry point: load corpus and convert to training format\n",
        "        return self.build_word_frequencies(corpus)"
      ],
      "metadata": {
        "id": "LbVHRbOREuNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "class VocabularyManager:\n",
        "    def __init__(self, end_of_word_token: str = \"</w>\"):\n",
        "        self.end_of_word_token = end_of_word_token\n",
        "        self.base_vocab = {}\n",
        "\n",
        "    def initialize_base_vocab(self, word_frequencies: dict[tuple[str, ...], int]) -> None:\n",
        "        # Create initial vocabulary from unique characters in corpus\n",
        "        unique_symbols = set()\n",
        "        for word in word_frequencies:\n",
        "          unique_symbols.update(word)\n",
        "\n",
        "        self.base_vocab = {sym: idx for idx, sym in enumerate(sorted(unique_symbols))}\n",
        "\n",
        "    def add_merge_token(self, token: str) -> int:\n",
        "        # Add new merged token to vocabulary and assign ID\n",
        "        token_id = len(self.base_vocab)\n",
        "        self.base_vocab[token] = token_id\n",
        "        return token_id\n",
        "\n",
        "    def get_token_id(self, token: str) -> Optional[int]:\n",
        "        # Look up token ID by token string\n",
        "        return self.base_vocab.get(token)\n",
        "\n",
        "    def get_token_by_id(self, token_id: int) -> Optional[str]:\n",
        "        # Look up token string by ID\n",
        "        for token, id_ in self.base_vocab.items():\n",
        "          if id_ == token_id:\n",
        "            return token\n",
        "        return None\n",
        "\n",
        "    def get_vocab(self) -> dict[str, int]:\n",
        "        # Return complete token→ID mapping\n",
        "        return self.base_vocab\n",
        "\n",
        "    def get_reverse_vocab(self) -> dict[int, str]:\n",
        "        # Return complete ID→token mapping\n",
        "        return {v: k for k, v in self.base_vocab.items()}\n",
        "\n",
        "    def vocab_size(self) -> int:\n",
        "        return len(self.base_vocab)\n",
        "\n",
        "    def contains_token(self, token: str) -> bool:\n",
        "        return token in self.base_vocab"
      ],
      "metadata": {
        "id": "SYRnIWSZKQmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class PairFrequencyAnalyzer:\n",
        "    def __init__(self, tie_breaking: str = \"lexicographic\"):\n",
        "        self.tie_breaking = tie_breaking\n",
        "\n",
        "    def count_all_pairs(self, word_frequencies: dict[tuple[str, ...], int]) -> dict[tuple[str, str], int]:\n",
        "        # Count frequency of every adjacent token pair across corpus\n",
        "        pair_counts = defaultdict(int)\n",
        "        for word, freq in word_frequencies.items():\n",
        "          pairs = self.get_pairs_from_word(word)\n",
        "          for pair in pairs:\n",
        "            pair_counts[pair] += freq\n",
        "        return dict(pair_counts)\n",
        "\n",
        "    def get_pairs_from_word(self, word: tuple[str, ...]) -> List[tuple[str, str]]:\n",
        "        # Extract all adjacent pairs from a single word\n",
        "        return [(word[i], word[i+1]) for i in range(len(word)-1)]\n",
        "\n",
        "    def find_most_frequent_pair(self, pair_counts: dict[tuple[str, str], int]) -> Optional[Tuple[str, str]]:\n",
        "        # Identify the pair to merge next (handles ties)\n",
        "        if not pair_counts:\n",
        "          return None\n",
        "\n",
        "        max_count = max(pair_counts.values())\n",
        "        candidates = [pair for pair, count in pair_counts.items() if count == max_count]\n",
        "\n",
        "        if len(candidates) == 1:\n",
        "          return candidates[0]\n",
        "        else:\n",
        "          return self.resolve_tie(candidates)\n",
        "\n",
        "    def resolve_tie(self, tied_pairs: list[tuple[str, str]]) -> tuple[str, str]:\n",
        "        # Apply tie-breaking strategy (lexicographic/random/etc.)\n",
        "        if self.tie_breaking == 'lexicographic':\n",
        "          return sorted(tied_pairs)[0]\n",
        "        else:\n",
        "          import random\n",
        "          return random.choice(tied_pairs)\n",
        "\n",
        "    def update_pair_counts_after_merge(self, word_frequencies: dict[tuple[str, ...], int],\n",
        "                                     merged_pair: tuple[str, str]) -> dict[tuple[str, str], int]:\n",
        "        # Efficiently recalculate pair frequencies after a merge\n",
        "        new_word_freqs = {}\n",
        "        for word, freq in word_frequencies.items():\n",
        "          new_word = []\n",
        "          i = 0\n",
        "          while i < len(word):\n",
        "            # merge pair\n",
        "            if i < len(word)-1 and (word[i], word[i+1]) == merged_pair:\n",
        "              new_word.append(word[i] + word[i+1])\n",
        "              i+=2\n",
        "            else:\n",
        "              new_word.append(word[i])\n",
        "              i+=1\n",
        "          new_word_freqs[tuple(new_word)] = freq\n",
        "        return self.count_all_pairs(new_word_freqs)"
      ],
      "metadata": {
        "id": "ZO9yPaNPWWiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MergeEngine:\n",
        "    def __init__(self):\n",
        "        self.merges = [] # list of tuples: ((A, B), 'AB')\n",
        "\n",
        "    def create_merge_token(self, pair: Tuple[str, str]) -> str:\n",
        "        # Generate new token name from pair (e.g., ('e','s') → 'es')\n",
        "        return pair[0] + pair[1]\n",
        "\n",
        "    def apply_merge_to_word(self, word: Tuple[str, ...], pair: Tuple[str, str],\n",
        "                           new_token: str) -> Tuple[str, ...]:\n",
        "        # Replace pair occurrences in single word\n",
        "        new_word = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "          if i < len(word) - 1 and (word[i], word[i+1]) == pair:\n",
        "            new_word.append(new_token)\n",
        "            i += 2\n",
        "          else:\n",
        "            new_word.append(word[i])\n",
        "            i += 1\n",
        "        return tuple(new_word)\n",
        "\n",
        "    def apply_merge_to_corpus(self, word_frequencies: Dict[Tuple[str, ...], int],\n",
        "                             pair: Tuple[str, str], new_token: str) -> Dict[Tuple[str, ...], int]:\n",
        "        # Apply merge across entire corpus\n",
        "        new_word_freqs = {}\n",
        "        for word, freq in word_frequencies.items():\n",
        "          merged_word = self.apply_merge_to_word(word, pair, new_token)\n",
        "          new_word_freqs[merged_word] = freq\n",
        "        return new_word_freqs\n",
        "\n",
        "    def get_merge_rules(self) -> List[Tuple[Tuple[str, str], str]]:\n",
        "        # Return ordered list of all merges performed\n",
        "        return self.merges\n",
        "\n",
        "    def add_merge_rule(self, pair: Tuple[str, str], new_token: str) -> None:\n",
        "        # Record a merge operation for later use\n",
        "        return self.merges.append((pair, new_token))"
      ],
      "metadata": {
        "id": "b86oXSFcbi2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BPETrainer:\n",
        "    def __init__(self, target_vocab_size: int, preprocessor: TextPreprocessor,\n",
        "                 vocab_manager: VocabularyManager, pair_analyzer: PairFrequencyAnalyzer,\n",
        "                 merge_engine: MergeEngine):\n",
        "      self.target_vocab_size = target_vocab_size\n",
        "      self.preprocessor = preprocessor\n",
        "      self.vocab_manager = vocab_manager\n",
        "      self.pair_analyzer = pair_analyzer\n",
        "      self.merge_engine = merge_engine\n",
        "\n",
        "    def train_from_corpus(self, corpus: List[str], verbose: bool = False) -> Tuple[Dict[str, int], List[Tuple[Tuple[str, str], str]]]:\n",
        "        # Main training entry point - orchestrates entire process\n",
        "        word_freqs = self.preprocessor.preprocess_corpus(corpus)\n",
        "        self.vocab_manager.initialize_base_vocab(word_freqs)\n",
        "        merge_rules = self.perform_merge_iterations(word_freqs, verbose=verbose)\n",
        "        return self.vocab_manager.get_vocab(), merge_rules\n",
        "\n",
        "    def perform_merge_iterations(self, word_frequencies: Dict[Tuple[str, ...], int],\n",
        "                               verbose: bool = False) -> List[Tuple[Tuple[str, str], str]]:\n",
        "        # Execute iterative merging until vocabulary target reached\n",
        "        merge_rules = []\n",
        "        while self.should_continue_merging(self.vocab_manager.vocab_size(), self.pair_analyzer.count_all_pairs(word_frequencies)):\n",
        "          pair_counts = self.pair_analyzer.counts_all_pair(word_frequencies)\n",
        "          best_pair = self.pair_analyzer.find_most_frequent_pairs(pair_counts)\n",
        "          if not best_pair:\n",
        "            break\n",
        "          new_token = self.merge_engine.create_merge_token(best_pair)\n",
        "          self.vocab_manager.add_merge_token(new_token)\n",
        "          self.merge_engine.add_merge_rule(best_pair, new_token)\n",
        "          word_frequencies = self.merge_engine.apply_merge_to_corpus(word_frequencies, best_pair, new_token)\n",
        "          if verbose:\n",
        "            self.log_merge_progress(iteration=len(merge_rules)+1, pair=best_pair,\n",
        "                                    frequency=pair_counts[best_pair],\n",
        "                                    vocab_size=self.vocab_manager.vocab_size())\n",
        "          merge_rules.append((best_pair, new_token))\n",
        "        return merge_rules\n",
        "\n",
        "    def should_continue_merging(self, current_vocab_size: int, pair_counts: Dict[Tuple[str, str], int]) -> bool:\n",
        "        # Determine if training should continue\n",
        "        if current_vocab_size >= self.target_vocab_size:\n",
        "          return False\n",
        "        if not pair_counts:\n",
        "          return False\n",
        "        return True\n",
        "\n",
        "    def log_merge_progress(self, iteration: int, pair: Tuple[str, str], frequency: int,\n",
        "                          vocab_size: int) -> None:\n",
        "        # Display training progress information\n",
        "        print(f\"Iteration {iteration}: Merged pair {pair} (freq={frequency}), vocab_size={vocab_size}\")"
      ],
      "metadata": {
        "id": "TJG73W4dex44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BPEEncoder:\n",
        "    def __init__(self, vocab: Dict[str, int], merge_rules: List[Tuple[Tuple[str, str], str]],\n",
        "                 preprocessor: TextPreprocessor):\n",
        "        self.vocab = vocab\n",
        "        self.merge_rules = merge_rules\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def encode_text(self, text: str) -> List[int]:\n",
        "        # Main encoding entry point - text to token IDs\n",
        "        words = self.preprocessor.tokenize_to_words(text)\n",
        "        token_ids = []\n",
        "        for word in words:\n",
        "          token_ids.extend(self.encode_word(word))\n",
        "        return token_ids\n",
        "\n",
        "    def encode_word(self, word: str) -> List[int]:\n",
        "        # Encode single word using BPE rules\n",
        "        word_chars = tuple(list(word) + [self.preprocessor.end_of_word_token])\n",
        "        tokens = self.apply_bpe_to_word(word_chars)\n",
        "        tokens = self.handle_unknown_tokens(tokens)\n",
        "        return self.tokens_to_ids(tokens)\n",
        "\n",
        "    def apply_bpe_to_word(self, word_chars: Tuple[str, ...]) -> List[str]:\n",
        "        # Apply merge rules to segment word into subwords\n",
        "        tokens = list(word_chars)\n",
        "        return self.apply_merge_rules(tokens)\n",
        "\n",
        "    def apply_merge_rules(self, tokens: List[str]) -> List[str]:\n",
        "        # Apply learned merges in training order\n",
        "        for pair, merged_token in self.merge_rules:\n",
        "          i = 0\n",
        "          while i < len(tokens) - 1:\n",
        "            if (tokens[i], tokens[i+1]) == pair:\n",
        "              tokens[i] = merged_token\n",
        "              del tokens[i+1]\n",
        "            else:\n",
        "              i +=1\n",
        "\n",
        "    def tokens_to_ids(self, tokens: List[str]) -> List[int]:\n",
        "        # Convert token strings to vocabulary IDs\n",
        "        return [self.vocab[token] for token in tokens if token in self.vocab]\n",
        "\n",
        "    def handle_unknown_tokens(self, tokens: List[str]) -> List[str]:\n",
        "        # Handle tokens not in vocabulary (fallback to characters)\n",
        "        result = []\n",
        "        for t in tokens:\n",
        "          if t in self.vocab:\n",
        "            result.append(t)\n",
        "          else:\n",
        "            result.extend(list(t))\n",
        "        return result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "qM-l2XyCh1X2",
        "outputId": "599325ac-2e49-4391-8052-9d440193d2be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after 'else' statement on line 36 (ipython-input-226833072.py, line 38)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-226833072.py\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    def tokens_to_ids(self, tokens: List[str]) -> List[int]:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'else' statement on line 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BPEDecoder:\n",
        "    def __init__(self, id_to_token: Dict[int, str], end_of_word_token: str = \"</w>\"):\n",
        "        self.id_to_token = id_to_token\n",
        "        self.end_of_word_token = end_of_word_token\n",
        "\n",
        "    def decode_ids(self, token_ids: List[int]) -> str:\n",
        "        # Main decoding entry point - token IDs to text\n",
        "        tokens = self.ids_to_tokens(token_ids)\n",
        "        words = self.reconstruct_words(tokens)\n",
        "        return self.join_words(words)\n",
        "\n",
        "    def ids_to_tokens(self, token_ids: List[int]) -> List[str]:\n",
        "        # Convert IDs to token strings\n",
        "        return [self.id_to_token[i] for i in token_ids if i in self.id_to_token]\n",
        "\n",
        "    def merge_subword_tokens(self, tokens: List[str]) -> str:\n",
        "        # Join subword tokens back into readable text\n",
        "        return \"\".join(token.replace(self.end_of_word_token, \"\") for token in tokens)\n",
        "\n",
        "    def reconstruct_words(self, tokens: List[str]) -> List[str]:\n",
        "        # Group subword tokens back into complete words\n",
        "        words = []\n",
        "        current_words = []\n",
        "\n",
        "        for token in tokens:\n",
        "          if token.endswith(self.end_of_word_token):\n",
        "            token_clean = token.replace(self.end_of_word_token, \"\")\n",
        "            current_word.append(token_clean)\n",
        "            words.append(\"\".join(current_word))\n",
        "          else:\n",
        "            current_word.append(token)\n",
        "\n",
        "        if current_word:\n",
        "          words.append(\"\".join(current_word))\n",
        "        return words\n",
        "\n",
        "    def join_words(self, words: List[str]) -> str:\n",
        "        # Join words with appropriate spacing\n",
        "        return \" \".join(words)"
      ],
      "metadata": {
        "id": "VX9HjYWajT3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import Dict, List, Tuple, Any\n",
        "\n",
        "class ModelSerializer:\n",
        "    @staticmethod\n",
        "    def save_model(vocab: Dict[str, int], merge_rules: List[Tuple[Tuple[str, str], str]],\n",
        "                   config: Dict[str, Any], filepath: str) -> None:\n",
        "        merge_rules_serializable = [[list(pair), merged] for pair, merged in merge_rules]\n",
        "        data = {\n",
        "            \"vocab\": vocab,\n",
        "            \"merge_rules\": merge_rules_serializable,\n",
        "            \"config\": config\n",
        "        }\n",
        "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_model(filepath: str) -> Tuple[Dict[str, int], List[Tuple[Tuple[str, str], str]], Dict[str, Any]]:\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "        merge_rules = [(tuple(pair), merged) for pair, merged in data[\"merge_rules\"]]\n",
        "        return data[\"vocab\"], merge_rules, data[\"config\"]\n",
        "\n",
        "    @staticmethod\n",
        "    def save_vocab_only(vocab: Dict[str, int], filepath: str) -> None:\n",
        "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(vocab, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vocab_only(filepath: str) -> Dict[str, int]:\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    @staticmethod\n",
        "    def export_merge_rules(merge_rules: List[Tuple[Tuple[str, str], str]], filepath: str) -> None:\n",
        "        merge_rules_serializable = [[list(pair), merged] for pair, merged in merge_rules]\n",
        "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(merge_rules_serializable, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    @staticmethod\n",
        "    def import_merge_rules(filepath: str) -> List[Tuple[Tuple[str, str], str]]:\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            merge_rules_serializable = json.load(f)\n",
        "        return [(tuple(pair), merged) for pair, merged in merge_rules_serializable]"
      ],
      "metadata": {
        "id": "Yvnh1elYlcYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BPETokenizer:\n",
        "    def __init__(self, vocab_size: int = 10000, end_of_word_token: str = \"</w>\",\n",
        "                 tie_breaking: str = \"lexicographic\"):\n",
        "      self.vocab_size = vocab_size\n",
        "      self.end_of_word_token = end_of_word_token\n",
        "      self.tie_breaking = tie_breaking\n",
        "\n",
        "      # Components\n",
        "      self.preprocessor = TextPreprocessor(end_of_word_token=self.end_of_word_token)\n",
        "      self.vocab_manager = VocabularyManager(end_of_word_token=self.end_of_word_token)\n",
        "      self.pair_analyzer = PairFrequencyAnalyzer(tie_breaking=self.tie_breaking)\n",
        "      self.merge_engine = MergeEngine()\n",
        "      self.trainer = BPETrainer(\n",
        "          target_vocab_size=self.vocab_size,\n",
        "          preprocessor=self.preprocessor,\n",
        "          vocab_manager=self.vocab_manager,\n",
        "          pair_analyzer=self.pair_analyzer,\n",
        "          merge_engine=self.merge_engine\n",
        "      )\n",
        "\n",
        "      # Encoder/Decoder placeholders\n",
        "      self.encoder = None\n",
        "      self.decoder = None\n",
        "      self.config = {\n",
        "          \"vocab_size\": self.vocab_size,\n",
        "          \"end_of_word_token\": self.end_of_word_token,\n",
        "          \"tie_breaking\": self.tie_breaking\n",
        "      }\n",
        "\n",
        "    def train(self, corpus: List[str], verbose: bool = False) -> None:\n",
        "        # Load and process text corpus, train BPE model\n",
        "        vocab, merge_rules = self.trainer.train_from_corpus(corpus, verbose=verbose)\n",
        "        self.encoder = BPEEncoder(vocab=vocab, merge_rules=merge_rules, preprocessor=self.preprocessor)\n",
        "        id_to_token = {i: tok for tok, i in vocab.items()}\n",
        "        self.decoder = BPEDecoder(id_to_token=id_to_token, end_of_word_token=self.end_of_word_token)\n",
        "        self.merge_rules = merge_rules\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        # Convert input text to token IDs using trained model\n",
        "        if self.encoder is None:\n",
        "            raise ValueError(\"Tokenizer not trained or loaded\")\n",
        "        return self.encoder.encode_text(text)\n",
        "\n",
        "    def decode(self, token_ids: List[int]) -> str:\n",
        "        # Convert token IDs back to readable text\n",
        "        if self.decoder is None:\n",
        "            raise ValueError(\"Tokenizer not trained or loaded\")\n",
        "        return self.decoder.decode_ids(token_ids)\n",
        "\n",
        "    def encode_batch(self, texts: List[str]) -> List[List[int]]:\n",
        "        # Batch encoding for efficiency\n",
        "        return [self.encode(t) for t in texts]\n",
        "\n",
        "    def decode_batch(self, token_ids_batch: List[List[int]]) -> List[str]:\n",
        "        # Batch decoding for efficiency\n",
        "        return [self.decode(ids) for ids in token_ids_batch]\n",
        "\n",
        "    def get_vocab_size(self) -> int:\n",
        "        return self.vocab_manager.vocab_size() if self.vocab_manager else 0\n",
        "\n",
        "    def save(self, filepath: str) -> None:\n",
        "        # Persist trained model to disk\n",
        "        if self.encoder is None or self.decoder is None:\n",
        "            raise ValueError(\"Tokenizer not trained yet\")\n",
        "        ModelSerializer.save_model(\n",
        "            vocab=self.encoder.vocab,\n",
        "            merge_rules=self.merge_rules,\n",
        "            config=self.config,\n",
        "            filepath=filepath\n",
        "        )\n",
        "\n",
        "    def load(self, filepath: str) -> None:\n",
        "        # Load pre-trained model from disk\n",
        "        vocab, merge_rules, config = ModelSerializer.load_model(filepath)\n",
        "        self.config = config\n",
        "        self.encoder = BPEEncoder(vocab=vocab, merge_rules=merge_rules, preprocessor=self.preprocessor)\n",
        "        id_to_token = {i: tok for tok, i in vocab.items()}\n",
        "        self.decoder = BPEDecoder(id_to_token=id_to_token, end_of_word_token=self.end_of_word_token)\n",
        "        self.merge_rules = merge_rules"
      ],
      "metadata": {
        "id": "Osfdh8TwleX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Back to book code**"
      ],
      "metadata": {
        "id": "H60Bs5tfOUYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRq5sEkWOS3x",
        "outputId": "b660c969-9c35-4757-d5fa-0488df72f543"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 198ms\u001b[0m\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "import tiktoken\n",
        "version('tiktoken')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "09S4BmkcOwHx",
        "outputId": "a89ee4a8-1933-4c28-b4d2-816792fccf08"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.11.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "0wrMK37NPtXf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "     \"of someunknownPlace.\"\n",
        ")\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJxKTBC5QKrC",
        "outputId": "0e4b047a-ee18-42e9-8649-f6b54f26a184"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "strings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "gofNCEpSQY0M",
        "outputId": "8f5b765b-4a09-4071-d31c-17e72831c1d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "len(enc_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puvKeLXlRHbE",
        "outputId": "8faabf4a-f752-40a3-dece-2f01b37433f7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5145"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample = enc_text[50:]"
      ],
      "metadata": {
        "id": "UnZatSFAWK7e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRR-80LwWT2H",
        "outputId": "e8b5f8bf-b9c9-49a7-a713-47060024e0f0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [290, 4920, 2241, 287]\n",
            "y:      [4920, 2241, 287, 257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "  context = enc_sample[:i]\n",
        "  desired = enc_sample[i]\n",
        "  print(context, \"---->\", desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhTTUWNcWwnw",
        "outputId": "abd0d22a-428d-4650-d7b5-194a23fcba84"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290] ----> 4920\n",
            "[290, 4920] ----> 2241\n",
            "[290, 4920, 2241] ----> 287\n",
            "[290, 4920, 2241, 287] ----> 257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Defj4nJAXLUC",
        "outputId": "b5a1b0de-8d46-4574-e972-31c488006c82"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and ---->  established\n",
            " and established ---->  himself\n",
            " and established himself ---->  in\n",
            " and established himself in ---->  a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "    token_ids = tokenizer.encode(txt)\n",
        "\n",
        "    for i in range(0, len(token_ids)-max_length, stride):\n",
        "      input_chunk = token_ids[i: i+max_length]\n",
        "      target_chunk = token_ids[i+1 : i+max_length+1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "XFce4ELpXfwH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "  tokenizer = tiktoken.get_encoding('gpt2')\n",
        "  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size = batch_size,\n",
        "      drop_last=drop_last,\n",
        "      num_workers=num_workers\n",
        "  )\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "nIde41_EVH5F"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "curyVfJKXRM2",
        "outputId": "d7595a3e-9147-4d25-d95b-f4808d2f1872"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "second_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKKrJ4KPXqoP",
        "outputId": "adcac387-d135-4956-b7fa-31f62e580f8b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=4, stride=4\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAhHoktWZ4zk",
        "outputId": "b9eb2ea1-7db6-4e09-b3d3-b7d266e84b26"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor([2, 3, 5, 1])\n",
        "vocab_size = 6\n",
        "output_dim = 3"
      ],
      "metadata": {
        "id": "8DhoDh_faewT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "embedding_layer.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHUI9FD-cEkc",
        "outputId": "60539f01-4224-4a7b-d40b-62af285faeb1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.3374, -0.1778, -0.1690],\n",
              "        [ 0.9178,  1.5810,  1.3010],\n",
              "        [ 1.2753, -0.2010, -0.1606],\n",
              "        [-0.4015,  0.9666, -1.1481],\n",
              "        [-1.1589,  0.3255, -0.6315],\n",
              "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lil0x2OAcTOi",
        "outputId": "4f81b8b6-0fad-4838-d5cf-7e72a650493e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NxogC3zhqyT",
        "outputId": "26d1e502-56cd-4c0a-9bb1-ca2c2e8342c0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=50257\n",
        "output_dim=256\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "6yx0HytojHiB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length=4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=4, stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3ZUiYJKsXL5",
        "outputId": "03da5c08-6c98-4629-b83a-9296a5d1780a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "token_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP-8Vy8MsvN3",
        "outputId": "fd2b5f0f-5fa2-4cf3-b5cc-95a5dd24c14e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 4, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "pos_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6OCn3Njs9uB",
        "outputId": "25fcb28c-fbd8-450c-bc68-eba758e01d82"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "input_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG0jArDItQ-m",
        "outputId": "59b54275-1479-4526-89e5-9c65b8d0ef75"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 4, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}